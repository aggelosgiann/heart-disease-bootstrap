{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOycUi7fdP/s0+4csl70dsn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"2c96fc3a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767186843227,"user_tz":-120,"elapsed":3527,"user":{"displayName":"Άγγελος Γιαννής","userId":"11732087646930075685"}},"outputId":"248fded9-4764-4db4-9317-1d57ccf2a2e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 303 entries, 0 to 302\n","Data columns (total 14 columns):\n"," #   Column    Non-Null Count  Dtype  \n","---  ------    --------------  -----  \n"," 0   age       303 non-null    int64  \n"," 1   sex       303 non-null    int64  \n"," 2   cp        303 non-null    int64  \n"," 3   trestbps  303 non-null    int64  \n"," 4   chol      303 non-null    int64  \n"," 5   fbs       303 non-null    int64  \n"," 6   restecg   303 non-null    int64  \n"," 7   thalach   303 non-null    int64  \n"," 8   exang     303 non-null    int64  \n"," 9   oldpeak   303 non-null    float64\n"," 10  slope     303 non-null    int64  \n"," 11  ca        303 non-null    int64  \n"," 12  thal      303 non-null    int64  \n"," 13  target    303 non-null    int64  \n","dtypes: float64(1), int64(13)\n","memory usage: 33.3 KB\n"]},{"output_type":"execute_result","data":{"text/plain":["array([0, 2, 1, 3, 4])"]},"metadata":{},"execution_count":1}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# Loading the CSV data into a Pandas DataFrame from a URL\n","df = pd.read_csv('https://raw.githubusercontent.com/kb22/Heart-Disease-Prediction/master/dataset.csv')\n","\n","# Display the first few rows of the DataFrame to inspect the data\n","df.head()\n","\n","# Display concise summary of the DataFrame, including data types and non-null values\n","df.info()\n","\n","# Display unique values for categorical features to understand their distribution\n","df[\"sex\"].unique()\n","df[\"cp\"].unique()\n","df[\"fbs\"].unique()\n","df[\"restecg\"].unique()\n","df[\"exang\"].unique()\n","df[\"slope\"].unique()\n","df[\"thal\"].unique()\n","df[\"ca\"].unique()"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.utils import resample\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import recall_score, accuracy_score\n","from scipy import stats\n","\n","# ==========================================\n","# 1. SETUP & DATA LOADING\n","# ==========================================\n","\n","# Basic Preprocessing\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Configuration\n","N_BOOTSTRAPS = 1000   # Number of bootstrap iterations (higher = more precise, e.g., 200 or 500)\n","RECALL_THRESHOLD = 0.90\n","RANDOM_SEED = 42\n","\n","print(f\"Starting Bootstrap Analysis with {N_BOOTSTRAPS} iterations...\")\n","\n","# ==========================================\n","# 2. BOOTSTRAP FUNCTION\n","# ==========================================\n","def get_bootstrap_metrics(model, X, y, seeds):\n","    \"\"\"\n","    Runs bootstrap evaluation using a fixed list of seeds.\n","    Returns arrays of Recall and Accuracy scores.\n","    \"\"\"\n","    recalls = []\n","    accuracies = []\n","\n","    for seed in seeds:\n","        # Bootstrap Resample (Create a training set with replacement)\n","        # The 'Out-of-Bag' (OOB) samples are used as the Test set\n","        train_idx = resample(np.arange(len(y)), replace=True, n_samples=len(y), random_state=seed)\n","        test_idx = np.setdiff1d(np.arange(len(y)), train_idx)\n","\n","        # Edge case: if test set is empty (rare), skip\n","        if len(test_idx) == 0: continue\n","\n","        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n","        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n","\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","\n","        recalls.append(recall_score(y_test, y_pred))\n","        accuracies.append(accuracy_score(y_test, y_pred))\n","\n","    return np.array(recalls), np.array(accuracies)\n","\n","# Generate fixed seeds so Baseline and Tuned models see the SAME data splits (Paired Testing)\n","bootstrap_seeds = np.random.RandomState(RANDOM_SEED).randint(0, 10000, size=N_BOOTSTRAPS)\n","\n","# ==========================================\n","# 3. DEFINE MODELS & GRIDS\n","# ==========================================\n","models = {\n","    'Logistic Regression': LogisticRegression(max_iter=2000),\n","    'SVC': SVC(probability=True),\n","    'KNN': KNeighborsClassifier(),\n","    'Random Forest': RandomForestClassifier()\n","}\n","\n","# Simplified Grids for demonstration (Expand these for better results)\n","param_grids = {\n","    'Logistic Regression': [{'C': x, 'class_weight': w} for x in [0.01, 0.1, 1, 10] for w in [None, 'balanced']],\n","    'SVC': [{'C': x, 'kernel': k, 'class_weight': 'balanced'} for x in [0.1, 1, 10] for k in ['rbf', 'linear']],\n","    'KNN': [{'n_neighbors': n} for n in [3, 5, 7, 9, 11]]\n","}\n","\n","results = {}\n","\n","# ==========================================\n","# 4. EXECUTION LOOP\n","# ==========================================\n","for name, base_model in models.items():\n","    print(f\"\\nAnalyzing {name}...\")\n","\n","    # Needs scaling? (Tree models don't, others do)\n","    if name == 'Random Forest':\n","        pipeline_base = base_model\n","    else:\n","        pipeline_base = make_pipeline(StandardScaler(), base_model)\n","\n","    # --- A. BASELINE EVALUATION ---\n","    # We must re-evaluate baseline with bootstrap to compare apples-to-apples\n","    base_rec, base_acc = get_bootstrap_metrics(pipeline_base, X, y, bootstrap_seeds)\n","\n","    # --- B. TUNING SEARCH (Find config with Recall >= 90%) ---\n","    best_tuned_acc = -1\n","    best_tuned_scores = None\n","    best_params = None\n","\n","    # Iterate through grid\n","    grid = param_grids.get(name, [])\n","    for params in grid:\n","        # Update model params\n","        if name == 'Random Forest':\n","            model_tune = base_model.set_params(**params)\n","            pipe_tune = model_tune\n","        else:\n","            # Update the classifier step in pipeline\n","            model_tune = base_model.set_params(**params)\n","            pipe_tune = make_pipeline(StandardScaler(), model_tune)\n","\n","        # Run Quick Bootstrap (fewer iterations for speed, e.g., 20) or Full\n","        # For accuracy, we'll use the full set here.\n","        rec_scores, acc_scores = get_bootstrap_metrics(pipe_tune, X, y, bootstrap_seeds)\n","\n","        mean_rec = np.mean(rec_scores)\n","\n","        # CRITERION: Average Recall >= 90%\n","        if mean_rec >= RECALL_THRESHOLD:\n","            mean_acc = np.mean(acc_scores)\n","            if mean_acc > best_tuned_acc:\n","                best_tuned_acc = mean_acc\n","                best_tuned_scores = (rec_scores, acc_scores)\n","                best_params = params\n","\n","    # --- C. STORE RESULTS ---\n","    results[name] = {\n","        'baseline_acc': base_acc,\n","        'baseline_rec': base_rec,\n","        'tuned_acc': best_tuned_scores[1] if best_tuned_scores else None,\n","        'tuned_rec': best_tuned_scores[0] if best_tuned_scores else None,\n","        'best_params': best_params\n","    }\n","\n","# ==========================================\n","# 5. STATISTICAL COMPARISON OUTPUT\n","# ==========================================\n","print(\"\\n\" + \"=\"*60)\n","print(f\"{'MODEL COMPARISON (BOOTSTRAP N=' + str(N_BOOTSTRAPS) + ')':^60}\")\n","print(\"=\"*60)\n","\n","for name in models.keys():\n","    res = results[name]\n","\n","    # Baseline Metrics\n","    base_acc_mean = np.mean(res['baseline_acc'])\n","    base_ci = np.percentile(res['baseline_acc'], [2.5, 97.5])\n","\n","    print(f\"\\n>>> {name}\")\n","    print(f\"   Baseline Accuracy: {base_acc_mean:.2%} (95% CI: {base_ci[0]:.2%} - {base_ci[1]:.2%})\")\n","\n","    if res['tuned_acc'] is not None:\n","        tuned_acc_mean = np.mean(res['tuned_acc'])\n","        tuned_rec_mean = np.mean(res['tuned_rec'])\n","        tuned_ci = np.percentile(res['tuned_acc'], [2.5, 97.5])\n","\n","        print(f\"   Tuned Accuracy:    {tuned_acc_mean:.2%} (95% CI: {tuned_ci[0]:.2%} - {tuned_ci[1]:.2%})\")\n","        print(f\"   (Constraint Met: Recall = {tuned_rec_mean:.2%})\")\n","        print(f\"   Best Params: {res['best_params']}\")\n","\n","        # --- STATISTICAL TEST ---\n","        # Paired t-test (Difference between Tuned and Baseline on SAME folds)\n","        diff = res['tuned_acc'] - res['baseline_acc']\n","        t_stat, p_val = stats.ttest_1samp(diff, 0)\n","\n","        significance = \"SIGNIFICANT\" if p_val < 0.05 else \"Not Significant\"\n","        print(f\"   Improvement:       {np.mean(diff):.2%} (p-value: {p_val:.4f}) -> {significance}\")\n","    else:\n","        print(f\"   [!] No configuration met the {RECALL_THRESHOLD*100}% Recall constraint.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2N317aoOVoK","executionInfo":{"status":"ok","timestamp":1767187690542,"user_tz":-120,"elapsed":837118,"user":{"displayName":"Άγγελος Γιαννής","userId":"11732087646930075685"}},"outputId":"811a6015-7497-40e7-d995-a1239d0c476e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting Bootstrap Analysis with 1000 iterations...\n","\n","Analyzing Logistic Regression...\n","\n","Analyzing SVC...\n","\n","Analyzing KNN...\n","\n","Analyzing Random Forest...\n","\n","============================================================\n","            MODEL COMPARISON (BOOTSTRAP N=1000)             \n","============================================================\n","\n",">>> Logistic Regression\n","   Baseline Accuracy: 81.65% (95% CI: 75.00% - 87.50%)\n","   Tuned Accuracy:    81.98% (95% CI: 75.65% - 87.74%)\n","   (Constraint Met: Recall = 91.19%)\n","   Best Params: {'C': 0.01, 'class_weight': None}\n","   Improvement:       0.33% (p-value: 0.0000) -> SIGNIFICANT\n","\n",">>> SVC\n","   Baseline Accuracy: 81.29% (95% CI: 75.22% - 87.28%)\n","   [!] No configuration met the 90.0% Recall constraint.\n","\n",">>> KNN\n","   Baseline Accuracy: 79.24% (95% CI: 72.48% - 85.71%)\n","   [!] No configuration met the 90.0% Recall constraint.\n","\n",">>> Random Forest\n","   Baseline Accuracy: 81.43% (95% CI: 75.23% - 87.39%)\n","   [!] No configuration met the 90.0% Recall constraint.\n"]}]}]}